import re
# Импортируем модуль для работы с коллекциями (для подсчета частот)
import collections

def main():
    """
    Основная функция программы
    """
    print("Программа для анализа текста")
    print("=" * 40)
    
    # Бесконечный цикл для запроса текста, пока пользователь не введет достаточно длинный текст
    while True:
        # Запрашиваем текст у пользователя
        text = input("Введите текст для анализа (не менее 100 символов):\n> ")
        
        # Удаляем лишние пробелы в начале и конце текста
        text = text.strip()
        
        # Проверяем длину текста
        if len(text) < 100:
            # Если текст слишком короткий, выводим сообщение и продолжаем цикл
            print("Текст слишком короткий! Пожалуйста, введите не менее 100 символов.\n")
        else:
            # Если текст достаточно длинный, выходим из цикла
            break
    
    print("\nОбрабатываю текст...")
    print("=" * 40)
    
    # ОБРАБОТКА ТЕКСТА
    
    # Приводим весь текст к нижнему регистру (делаем все буквы маленькими)
    text_lower = text.lower()
    
    # Удаляем знаки препинания используя регулярные выражения
    # [^\w\s] означает: все символы, которые НЕ являются буквами/цифрами (\w) и пробелами (\s)
    # replace(" - ", " ") заменяет дефисы с пробелами вокруг на обычные пробелы
    text_clean = re.sub(r'[^\w\s]', '', text_lower.replace(" - ", " "))
    
    # РАЗБИЕНИЕ НА СЛОВА
    
    # Разбиваем очищенный текст на слова (разделитель - пробел)
    words = text_clean.split()
    
    # ПОДСЧЕТ СТАТИСТИКИ
    
    # 1. Подсчет общего количества символов
    total_chars_with_spaces = len(text)  # Все символы включая пробелы
    total_chars_without_spaces = len(text.replace(" ", ""))  # Символы без пробелов
    
    # 2. Количество словоформ (уникальных слов)
    word_count = len(words)
    
    # 3. Поиск 5 самых частых словоформ
    # Используем Counter для подсчета частоты каждого слова
    word_freq = collections.Counter(words)
    # most_common(5) возвращает 5 самых частых слов
    most_common_words = word_freq.most_common(5)
    
    # 4. Поиск 5 самых длинных словоформ
    # Создаем множество для удаления дубликатов
    unique_words = set(words)
    # Сортируем слова по длине в убывающем порядке и берем первые 5
    longest_words = sorted(unique_words, key=len, reverse=True)[:5]
    
    # 5. Подсчет средней длины словоформы
    # Суммируем длины всех слов и делим на количество слов
    total_length = sum(len(word) for word in words)
    average_length = total_length / word_count if word_count > 0 else 0
    
    # ВЫВОД РЕЗУЛЬТАТОВ
    
    print("\nРезультаты анализа:")
    print("=" * 40)
    
    # Выводим общее количество символов
    print(f"Общее количество символов: {total_chars_with_spaces} (без пробелов: {total_chars_without_spaces})")
    
    # Выводим количество словоформ
    print(f"Количество словоформ: {word_count}")
    
    # Выводим самые частые словоформы
    print("Самые частые словоформы:")
    for word, freq in most_common_words:
        # Правильно склоняем слово "раз" в зависимости от частоты
        times_word = "раз" if freq == 1 else "раза" if 2 <= freq <= 4 else "раз"
        print(f"- '{word}': {freq} {times_word}")
    
    # Выводим самые длинные словоформы
    print("Самые длинные словоформы:")
    for word in longest_words:
        print(f"- '{word}' ({len(word)} букв)")
    
    # Выводим среднюю длину словоформы (округляем до 1 знака после запятой)
    print(f"Средняя длина словоформы: {average_length:.1f} символа")

# Проверяем, запущен ли скрипт напрямую (а не импортирован)
if __name__ == "__main__":
    # Запускаем основную функцию
    main()